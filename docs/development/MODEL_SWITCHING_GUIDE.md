# üéØ Model Switching Guide

## Quick Start: Modell wechseln

**JA, du kannst Modelle einfach in der .env wechseln!** Hier ist wie:

```bash
# In .env - einfach √§ndern:
OPENAI_MODEL=gpt-4o-mini        # Empfohlen f√ºr Produktion
OPENAI_MODEL=gpt-3.5-turbo      # G√ºnstig und schnell
OPENAI_MODEL=gpt-4-turbo        # Beste Qualit√§t
OPENAI_MODEL=gpt-4o             # Neuestes Modell
```

**Das war's! Kein Code √§ndern, kein Neustart n√∂tig.**

## üìä Verf√ºgbare Modelle & Features

| Modell | JSON Format | Context Window | Kosten | Empfehlung |
|--------|------------|----------------|--------|------------|
| **gpt-4o** | ‚úÖ Ja | 128k tokens | $5/$15 | Beste Balance |
| **gpt-4o-mini** | ‚úÖ Ja | 128k tokens | $0.15/$0.60 | **‚≠ê EMPFOHLEN** |
| **gpt-4-turbo** | ‚úÖ Ja | 128k tokens | $10/$30 | H√∂chste Qualit√§t |
| **gpt-4-turbo-preview** | ‚úÖ Ja | 128k tokens | $10/$30 | Experimentell |
| **gpt-3.5-turbo** | ‚úÖ Ja | 16k tokens | $0.50/$1.50 | Budget-Option |
| **gpt-3.5-turbo-16k** | ‚úÖ Ja | 16k tokens | $3/$4 | Gro√üe Dokumente |

*Kosten: $ pro Million Input/Output Tokens*

## ‚öôÔ∏è Weitere Einstellungen (optional)

### 1. Temperature anpassen (Kreativit√§t)
```bash
# In .env hinzuf√ºgen:
OPENAI_TEMPERATURE=0.1   # 0.0-1.0 (niedrig = pr√§ziser)
```

### 2. Max Tokens √§ndern
```bash
# In .env hinzuf√ºgen:
OPENAI_MAX_TOKENS=2000   # Maximale Antwortl√§nge
```

### 3. Context Window f√ºr gro√üe Dokumente
```python
# In metadata_extraction.py Zeile 48:
def _truncate_text_for_llm(self, text: str, max_chars: int = 40000):
    # √Ñndern zu:
    # max_chars: int = 100000  # F√ºr gpt-4 Modelle mit 128k context
```

## üö® Wichtige Hinweise

### "gpt-5-nano" und "gpt-5-mini"
Diese Modelle **existieren nicht**! OpenAI hat keine GPT-5 Modelle ver√∂ffentlicht.
Falls du diese in .env siehst, √§ndere sie zu echten Modellen:

```bash
# FALSCH (funktioniert nicht richtig):
OPENAI_MODEL=gpt-5-nano  ‚ùå

# RICHTIG:
OPENAI_MODEL=gpt-4o-mini  ‚úÖ
```

### JSON Response Format
Der Code erkennt automatisch, ob ein Modell JSON unterst√ºtzt:
- ‚úÖ Unterst√ºtzt: gpt-4*, gpt-3.5-turbo* (automatisch aktiviert)
- ‚ùå Nicht unterst√ºtzt: Alte Modelle, Custom Models (automatisch deaktiviert)

### Fallback-Kette
```bash
# Provider-Reihenfolge in .env:
LLM_PROVIDER_ORDER=openai,ollama,openai_mini

# openai_mini verwendet automatisch gpt-4o-mini als Fallback
# Du musst nichts weiter konfigurieren!
```

## üß™ Modell testen

```bash
# Test mit aktuellem Modell:
source venv/bin/activate
python3 -c "
from src.paperless_ngx.infrastructure.config.settings import get_settings
s = get_settings()
print(f'Aktuelles Modell: {s.openai_model}')
print(f'Temperature: {s.openai_temperature}')
print(f'Max Tokens: {s.openai_max_tokens}')
"

# Workflow 2 mit neuem Modell testen:
python -m paperless_ngx.presentation.cli.simplified_menu --workflow 2
```

## üí° Empfehlungen nach Use Case

### F√ºr Produktion (Beste Balance)
```bash
OPENAI_MODEL=gpt-4o-mini
OPENAI_TEMPERATURE=0.1
OPENAI_MAX_TOKENS=2000
```

### F√ºr maximale Qualit√§t
```bash
OPENAI_MODEL=gpt-4-turbo
OPENAI_TEMPERATURE=0.2
OPENAI_MAX_TOKENS=3000
```

### F√ºr Budget-Betrieb
```bash
OPENAI_MODEL=gpt-3.5-turbo
OPENAI_TEMPERATURE=0.1
OPENAI_MAX_TOKENS=1500
```

### F√ºr gro√üe Dokumente (>50 Seiten)
```bash
OPENAI_MODEL=gpt-4o        # 128k context window
OPENAI_TEMPERATURE=0.1
OPENAI_MAX_TOKENS=4000
```

## üîß Troubleshooting

### Problem: "Primary JSON extraction failed"
**L√∂sung**: Wechsel zu einem Modell mit JSON-Support (gpt-4o-mini, gpt-3.5-turbo)

### Problem: "Context window exceeded"
**L√∂sung**: 
1. Nutze Modell mit gr√∂√üerem Context (gpt-4o, gpt-4-turbo)
2. Oder erh√∂he max_chars in metadata_extraction.py

### Problem: "Model not found"
**L√∂sung**: Pr√ºfe Schreibweise! Exakte Namen verwenden:
- ‚úÖ `gpt-4o-mini` (mit Bindestrich)
- ‚ùå `gpt4omini` (ohne Bindestriche)

## üìà Kosten-Monitoring

```bash
# Kosten-Tracking ist eingebaut! Check die Logs:
grep "cost" logs/*.log

# Oder in Python:
from src.paperless_ngx.infrastructure.llm.litellm_client import get_llm_client
client = get_llm_client()
print(client.cost_tracker.get_total_cost())
```

---

**Zusammenfassung**: Ja, du kannst Modelle einfach in .env wechseln! Keine weiteren Einstellungen n√∂tig. Der Code passt sich automatisch an.